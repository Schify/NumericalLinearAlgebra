\documentclass{article}
\usepackage[top=2cm, bottom=2cm, left=2.2cm, right=2.5cm]{geometry}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{enumerate}% http://ctan.org/pkg/enumerate

\usepackage[]{pdfcomment}
%As recommended by matlab2tikz
\usepackage{pgfplots}
\usepackage{tikz}
\usepackage{nicematrix}
\usepackage{calc}
\usepackage{pgfplotstable}
\usepackage{booktabs}
\usepackage{csvsimple}
\usepackage{etoolbox}
\usepackage{biblatex}
\bibliography{refs.bib}
%% the following commands are needed for some matlab2tikz features
\usetikzlibrary{plotmarks}
\usetikzlibrary{arrows.meta}
\usepgfplotslibrary{patchplots}
\usepackage{grffile}
%% you may also want the following commands
\pgfplotsset{plot coordinates/math parser=false}
%\newlength\figureheight
%\newlength\figurewidth
\usepackage{hyperref}
\usepackage{placeins,nicefrac}
\usepackage{xstring}
\usepackage[]{pgfkeys}
\usepackage{xfrac}
\usepackage{minted}
\usetikzlibrary{external}
\usetikzlibrary{calc,math}
\tikzexternalize[prefix=extern/]
\usepgfplotslibrary{groupplots}
\usepgfplotslibrary{colormaps}
\usetikzlibrary{decorations.markings,patterns,patterns.meta,pgfplots.fillbetween}

\usepackage[ruled,vlined,linesnumbered,algo2e]{algorithm2e}
\usepackage{wrapfig}
\usepackage{subcaption}
\graphicspath{../plots/}

\usepackage{wasysym}
\usepackage{animate}

%\usepackage{expl3}
%\ExplSyntaxOn
%\cs_set_eq:NN \fpeval \fp_eval:n
%\ExplSyntaxOff

\usetikzlibrary{fpu}
\graphicspath{{../figs/}}
\newcommand{\figurescale}{1.0}

%\usepackage{titlesec}
%\titleformat{\subsection}{\normalfont\large\bfseries}{Task \thesubsection}{1em}{}
%\titleformat{\section}{\normalfont\Large\bfseries}{Assignment \thesection}{1em}{}
%\titleformat{\subsubsection}{\normalfont\bfseries}{Question \thesubsubsection}{1em}{}

\newcommand{\stilltodo}[1]{{\color{red} UNFINISHED #1}}

\makeatletter
\newcommand{\trp}{%
	{\mathpalette\@transpose{}}%
}
\newcommand*{\@transpose}[2]{%
	% #1: math style
	% #2: unused
	\raisebox{\depth}{$\m@th#1\intercal$}%
}

\makeatother
\newcommand{\diff}{\mathrm{d}}
\author{Andr\'as Schifferer, r0915705}
\title{Numerical Linear Algebra [H03G1A]\\{\LARGE HW 1}}
\date{\today} 

\pgfplotsset{plot coordinates/math parser=false}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}


\begin{document}
	\maketitle
	%The assignment is not yet finished unfortunately.
	\tableofcontents
	
	
	
	\newpage
	\section{Semi-Orthogonality and strategic reorthogonalization}
	\textit{Note that throughout the first part of the assignment if an algorithm is performed on an unspecified matrix, then the subjected matrix should be understood to be the one provided for the assignment as \texttt{Test.mtx}.
	}
	
	The LÃ¡nczos algorithm can be used to construct a Hessenberg decomposition of a matrix plus a rank 1 term. If we restrict the matrix inputs to real symmetric matrices, then the symmetry requirement will force the apprximately similar Hessenberg matrix to be symmetric as well, which results necessarily in a tridiagonal matrix.
	\begin{algorithm2e}[ht]
		
		\SetKwInOut{Input}{input}
		\SetKwInOut{Output}{output}
		\SetKw{Init}{init}{}{}
		\SetAlgoLined
		\Input{Linear, symmetric real operator $A$ on $\mathbb{R}$}
		\Output{Approximately orthogonally similar tridiagonal $T\sim A$, given by diagonals $\mathbf{\alpha},\mathbf{\beta}$}
		\Init{choose starting vector $\mathbf{r}_0$,$\beta_0:=\|\mathbf{r}_0\|$, $\mathbf{q}_0=0$}\\
		\For{$j=1,2,...$}{
			$\mathbf{q}_j:=\mathbf{r}_{j-1}/\beta_{j-1}$\\
			$\mathbf{v}_j=A\mathbf{q}_j-\beta_{j-1}\mathbf{q}_{j-1}$\\
			$\alpha_j=\mathbf{v}_j^{\ast}\mathbf{q}_j$\\
			$\mathbf{r}_j=\mathbf{v}_j-\alpha_j\mathbf{q}_j$\\
			$\beta_j=\|\mathbf{r}_j\|$\\
		}
		\caption{Lanczos in exact arithmetic}\label{alg:Lanczos}
	\end{algorithm2e}\\
	
	The matrix $T$ then has the form 
	$$T=\begin{pmatrix}
		\alpha_1&\beta_1&0&\cdots&0\\
		\beta_1&\alpha_2&\beta_2&&0\\
		\vdots&\ddots&\ddots&\ddots&\vdots\\
		0&&\beta_{j-2}&\alpha_{j-1}&\beta_{j-1}\\
		0&\cdots&0&\beta_{j-1}&\alpha_{j}\\
	\end{pmatrix}$$
	One step in the main loop of \autoref{alg:Lanczos} is called a Lanczos step, and is usually written as
	\begin{equation}\label{eq:LanczosStep}
		\beta_j\mathbf{q}_{j+1}=A\mathbf{q}_j-\alpha_j\mathbf{q}_j-\beta_{j-1}\mathbf{q}_{j-1}.
	\end{equation}
%\part{title}	\inputminted[firstline=2, lastline=5]{matab}{../NLAHW1_Lanczos/test.m}
	
	We know that, with $Q_{j}:=[\mathbf{q}_1,\ldots,\mathbf{q}_j]$, we have the relation
	\begin{equation}\label{eq:matLanczos}
		AQ_{j}=Q_jT_j+\beta_{j}\mathbf{q}_{j+1}\mathbf{e}^{\ast}.
	\end{equation}
	In exact arithmetic the above is a great procedure for tridiagonalization. Numerically this is
	not the case unfortunately, as the orthogonality of $Q$ gets diminished by numerical errors.
	
	\subsection{Lanczos0: 'theoretical' Lanczos}
	
	The theoretical Lanczos algorithm (hereby reffered to as Lanczos0) will lose stability after only a small number of iterations, as examplified in \autoref{fig:lanczos0_W}
	\begin{figure}
		\centering
		\resizebox{\textwidth}{!}{
		\input{../plots/lanczos0_W.tex}}
		\caption{Loss of orthogonality in Lanczos 0}\label{fig:lanczos0_W}
	\end{figure}
	We can see that orthogonality is rapidlly lost at about iteration 20, and after that point a significant part of the former vectors are no longer orthogonal to the new ones.
	
	This can be further corrobarated by the convergence of the Ritz values as seen in \autoref{fig:lanczos0_W_ritz}. Note that the test matrix (provided in \texttt{Test.mtx}) only has real eigenvalues more than or equal to one. We can see that in 
	comparison with the better performing Lanczos1 algorithm (to be discussed later) where the Ritz values seem to be doing a good job approximating the spectrum of eigenvalues (in \autoref{fig:lanczos1_W_ritz}). In contrast Lanczos0 has trouble fully covering the eigenvalues in the Test matrix. Even at higher iteration numbers (i.e. a larger number of Ritz values) sometimes a spurious Ritz value appears, for example between the two largest eigenvalues at $j=39$. This might be explained by the last vector having a too high component parallel to the eigenvector corresponding to a larger eigenvalue. Therefore this might change the eigenvalues of $T_k$, which results in a bad quality approximation of the spectrum.
	
	
	
	\subsection{Lanczos1: 'practical' Lanczos}
	% It is not practical because calculating Q*Q is a lot of effort
	
	
	
	\begin{figure}
		\centering
		\resizebox{\textwidth}{!}{
			\input{../plots/lanczos1_W.tex}}
		\caption{Loss of orthogonality in Lanczos 1. Note that $w_{k,\infty}$ is always shown, as the one used in testing for reorthogonalization (i.e. the projected one in the next step)}\label{fig:lanczos1_W}
	\end{figure}
	
	\begin{figure}
	\centering
	\resizebox{0.95\textwidth}{!}{
		\includegraphics[trim=0 180 0 250 clip]{../plots/lanczos0_ritz.pdf}}
	\caption{Lanczos0: The eigenvalues of the test matrix ($\lambda_i$'s histogram) and the Ritz values at the $j^{\mathrm{th}}$ iteration ($\mu_i$, denoted by ${\color{red} \times}$ )}\label{fig:lanczos0_W_ritz}
	\end{figure}
	
	\begin{figure}
	\centering
	\resizebox{0.95\textwidth}{!}{
		\includegraphics[trim=0 170 0 180,clip]{../plots/lanczos1_ritz.pdf}}
	\caption{Lanczos1: The eigenvalues of the test matrix ($\lambda_i$'s histogram) and the Ritz values at the $j^{\mathrm{th}}$ iteration ($\mu_i$, denoted by ${\color{red} \times}$ )}\label{fig:lanczos1_W_ritz}
	\end{figure}
	
	\FloatBarrier
	
	\subsection{Lanczos2: heuristic for loss of orthogonality}
	
	Calculating $W = Q^\ast Q$ was the most costly part of the Lanczos1 algorithm, especially since this step needs to be done at each iteration, to determine whether a correction is needed or not in the first place. Loss of orthogonality could be predicted using a parallel system for iteratively constructing $\tilde{W}$, which could serve as a proxy to gauge when loss of orthogonality is to be expected. Our main assumption going forward will be that floating point errors have comaprable behaviour to that of Gaussian noise.
	
	Ideally in exact arithmetic $W_j = \mathbb{I}_{j\times j}$ (identity) would hold. However if at each iteration, i.e. the calculation of the new $\textbf{q}_j$, we could have access to the exact arithmetic error (which is assumed to be random and therefore inaccessible), we could predict the evolution of the non-ideal $W_j$. First let's look at the Lanczos step in \autoref{eq:LanczosStep}. By adding the floating point error $\textbf{f}_j$
	the equation becomes
	\begin{equation}\label{eq:LanczosStep-f}
		\beta_j\mathbf{q}_{j+1}=A\mathbf{q}_j-\alpha_j\mathbf{q}_j-\beta_{j-1}\mathbf{q}_{j-1} - \textbf{f}_j.
	\end{equation}
	This can be transformed in a similar fashion as before into 
	\begin{equation}
		AQ_{j}=Q_jT_j+\beta_{j}\mathbf{q}_{j+1}\mathbf{e}^{\ast} + F_j,
	\end{equation}
	where $F_j$ is a matrix collecting the error terms.
	
	We will assume, that at any given point the bases has remained sufficiently orthonormal, and therefore 
	\begin{align}
		\left\lVert \textbf{q}_k \right\rVert_2 &= 1,\\
		\beta_{k} \textbf{q}_k^\ast \textbf{q}_{k+1} &= C \epsilon \left\lVert A \right\rVert,
	\end{align}
	where $C$ is a modest constant. This presupposes the fact that we won't let the orthogonality to diminish too much.
	
	We can manipulate \autoref{eq:LanczosStep-f} by taking the inner product with $\textbf{q}_k$. Thereby
	\begin{equation}\label{eq:reccurence-mult}
		\beta_j\mathbf{q}_k^*\mathbf{q}_{j+1}=\mathbf{q}_k^*A\mathbf{q}_j-\alpha_j\mathbf{q}_k^*\mathbf{q}_j-\beta_{j-1}\mathbf{q}_k^*\mathbf{q}_{j-1} - \mathbf{q}_k^*\textbf{f}_j,
	\end{equation}
	holding true for $j=2,3,\dots J$ and $k=1,2, \dots J$. If we again define $W_J = Q_J^\ast Q_J$, and we denote the elements of this matrix as $w_{x,y}$, then a number of identities may follow, such as 
	\begin{align}
		w_{k,k} &=  1, \quad&&\mathrm{for}\,\, k = 1,2 \dots J\label{eq:wconds1}, \\
		w_{k,k-1} &= \underbrace{\textbf{q}_k^\ast \textbf{q}_{k-1}}_{\psi_k := } \quad&&\mathrm{for}\,\, k = 2,3 \dots J\label{eq:wconds2}, \\
		w_{j,k+1} &=  w_{k+1, j}\quad&&\mathrm{for}\,\, k = 1,2 \dots J-1\label{eq:wconds3}.
	\end{align}
	\autoref{eq:wconds1} is just a result of our stipulation that the system is orthonormal, and we are able to sufficiently normalize the vectors. \autoref{eq:wconds3} follows from the symmetry of $W_{J}$, which is inherent from its definition, as well as \autoref{eq:wconds2} evidently.
	
	Notice, that in \autoref{eq:reccurence-mult} the indicies can be swaped, so taking \autoref{eq:LanczosStep-f} at $k$ and multiplying it by $\textbf{q}_{j}$ we get
	\begin{equation}\label{eq:reccurence-mult-k}
		\beta_k\mathbf{q}_j^*\mathbf{q}_{k+1}=\mathbf{q}_j^*A\mathbf{q}_k-\alpha_k\mathbf{q}_j^*\mathbf{q}_k-\beta_{k-1}\mathbf{q}_j^*\mathbf{q}_{k-1} - \mathbf{q}_j^*\textbf{f}_k.
	\end{equation}
	Looking at the difference of \autoref{eq:reccurence-mult} and \autoref{eq:reccurence-mult-k}
	\begin{equation}
		\beta_j\mathbf{q}_k^*\mathbf{q}_{j+1} - \beta_k\mathbf{q}_j^*\mathbf{q}_{k+1}=\underbrace{\mathbf{q}_k^*A\mathbf{q}_j - \mathbf{q}_j^*A\mathbf{q}_k}_{=0} +
		\alpha_k\mathbf{q}_j^*\mathbf{q}_k - \alpha_j\mathbf{q}_k^*\mathbf{q}_j + \beta_{k-1}\mathbf{q}_j^*\mathbf{q}_{k-1} - \beta_{j-1}\mathbf{q}_k^*\mathbf{q}_{j-1} + \mathbf{q}_j^*\textbf{f}_k  - \mathbf{q}_k^*\textbf{f}_j,
	\end{equation}
	where $\mathbf{q}_k^*A\mathbf{q}_j - \mathbf{q}_j^*A\mathbf{q}_k = 0$ thanks to the symmetry of $A$. Thereby
	\begin{equation}
		\beta_j w_{k,j+1} = \beta_k w_{j,k+1} = \left(\alpha_k - \alpha_j\right) w_{k,j} + \beta_{k-1} w_{j, k-1} - \beta_{j-1} w_{k,j-1} + \underbrace{\mathbf{q}_j^*\textbf{f}_k-\mathbf{q}_k^*\textbf{f}_j}_{\theta_{j,k} 
		:=}.
	\end{equation}
	 
	 This update law of the $W_J$ matrix allows us to iteratively construct the matrix for $k=1,\dots j-1$ (stipulating that $w_{k,0} = 0$). Here we can assume, that $\psi_k$ and $\theta_{j,k}$ are just noise introduced by imperfect arithmetics, and as such can be stochastically modelled.
	 
\section{Eigenvalues of a tridiagonal matrix}
\subsection{Characteristic polynomial}
Suppose that a tridiagonal symmetric (real) matrix $T$ is given:

$$T=\begin{pmatrix}
	\alpha_1&\beta_1&0&\cdots&0\\
	\beta_1&\alpha_2&\beta_2&&0\\
	\vdots&\ddots&\ddots&\ddots&\vdots\\
	0&&\beta_{j-2}&\alpha_{j-1}&\beta_{j-1}\\
	0&\cdots&0&\beta_{j-1}&\alpha_{j}\\
\end{pmatrix}$$

Let $T_j := T (1 : j, 1 : j)$ as before. Now let $p_j(x)=\det\left(x I - T_j\right)$ denote the characteristic polynomial of $T_j$ (with $p_0 = 1$). A reccurence relation can easily be established using the recursive nature of the determinant and the matrix. We can write the $j^{\mathrm{th}}$ characteristic polynomial as
\begin{equation}
	p_j(x)=\det\left(x I - T_j\right) = \left|\begin{NiceMatrix}
		x-\alpha_1&-\beta_1&0&\cdots&0\\
		-\beta_1&x-\alpha_2&-\beta_2&&0\\
		\vdots&\ddots&\ddots&\ddots&\vdots\\
		0&&-\beta_{j-2}&x-\alpha_{j-1}&-\beta_{j-1}\\
		0&\cdots&0&-\beta_{j-1}&x-\alpha_{j}
		\CodeAfter
		\begin{tikzpicture}[remember picture, overlay]
			%\draw[->] (1-1.-20) to[out=-25, in=30] (4-3.30);
			\draw[red, opacity=0.3, fill opacity=0.05, dashed,rounded corners, fill=red] ($(1-1)!1.3!(1-1.north west)$) rectangle ($(4-4)!1.3!(4-4.south east)$);
			\node[red, opacity=0.5] at ($(1-4)!0.5!(4-4)$) {$xI-T_{j-1}$};
			
			\draw[blue, opacity=0.3, fill opacity=0.05, dashed,rounded corners, fill=blue] (1-1.north west) rectangle (4-3.south east);
			\draw[blue, opacity=0.3, fill opacity=0.05, dashed,rounded corners, fill=blue] ($(1-5.north)!(4-5.south west)!(1-1.north)$) rectangle (4-5.south east);
			\node[blue, opacity=0.5] (Ujm) at ($(1-4)+(0,0.3)$) {$U_{j-1}$};
			\draw [<-, blue, opacity=0.2](Ujm.160) to[in=30, out=150] (1-3);
			\draw [<-, blue, opacity=0.2](Ujm.20) to[in=150, out=40] (1-5);
		\end{tikzpicture}
	\end{NiceMatrix}\right| = \left(x-\alpha_{j}\right) \underbrace{\det{\left(xI-T_{j-1}\right)}}_{p_{j-1}} - \left(-\beta_{j-1}\right) \det{\left(U_{j-1}\right)}
\end{equation}
where we just used the so called Laplace expansion on the matrix while looking at the two non-zero elements of the last row. Since the determinant of $U_{j-1}$ can also be simply written as
\begin{equation*}
	\det{\left(U_{j-1}\right)} = \left|\begin{NiceMatrix}
		x-\alpha_1&-\beta_1&0&\cdots&0\\
		-\beta_1&x-\alpha_2&-\beta_2&&0\\
		\vdots&\ddots&\ddots&\ddots&\vdots\\
		0&&-\beta_{j-3}&x-\alpha_{j-2}&0\\
		0&\cdots&0&-\beta_{j-2}&-\beta_{j-1}
		\CodeAfter
		\begin{tikzpicture}[remember picture, overlay]
			\draw[orange, opacity=0.3, fill opacity=0.05, dashed,rounded corners, fill=orange] ($(1-1)!1.3!(1-1.north west)$) rectangle ($(4-4)!1.3!(4-4.south east)$);
			\node[red, opacity=0.5] at ($(1-4)!0.5!(4-4)$) {$xI-T_{j-2}$};
		\end{tikzpicture}
	\end{NiceMatrix}\right| = -\beta_{j-1} \underbrace{\det\left(xI-T_{j-2}\right)}_{p_{j-2}}
\end{equation*}
by expanding using the last row. Therefore using the definition of the characteristic polynomial we have our recursion in the form of 
\begin{equation}\label{eq:char-rec}
	p_j(x) = (\alpha_j - x)p_{j-1} (x) - \beta_{j-1}^2
	p_{j-2}(x).
\end{equation}

Hence for any given value of $x$, the resulting sequence of characteristic polynomials can be calculated using \autoref{alg:char-poly-eval}, where to get $p_n$ a loop of scalar multiplication, thereby having a complexity of $\mathcal{O}(n)$.

\begin{algorithm2e}[ht]
	
	\SetKwInOut{Input}{input}
	\SetKwInOut{Output}{output}
	\SetKw{Init}{init}{}{}
	\SetAlgoLined
	\Input{Real symmetric tridiagonal matrix $A\in\mathbb{R}^{n\times n}$, given by diagonals $\mathbf{\alpha},\mathbf{\beta}$, $x$ real constant}
	\Output{$\left\{p_j\right\}_{j=0}^n$ vector of the values of the characteristic polynomials evaluated at $x$}
	\Init{Allocate the $\left\{p_j\right\}_{j=0}^n$ vector, set the trivial cases: $p_0=1$, $p_1=\alpha_1-x$}\\
	\For{$j=2,3,...,n$}{
		$p_j = (\alpha_j - x)p_{j-1} - \beta_{j-1}^2
		p_{j-2}$\\
	}
	\caption{Evaluation of the characteristic polynomials ($p_j$ for $j=1,2,\dots,k$)}\label{alg:char-poly-eval}
\end{algorithm2e}

\subsection{Finding the kth eigenvalue}

Just by finding the zeros of the characteristic polynomial using the bisection method for example, by repeatedly evaluating it. This costs only $\mathcal{O}(n\log{n})$, disregarding some of the technical difficulties. With this algorithm however we would not be able to tell which eigenvalue we have converged to. 

For us to find a more useful algorithm we first have to remind ourselves of the Sturm sequence like property of the characteristic polynomials defined by
 \autoref{eq:char-rec}.
 \begin{theorem}[Sturm sequence property]
 	Suppose $T$ is such that $\{\beta_i\}_{i=1}^{n-1}$ contains no zeros. Then for any $j \in \{1,\ldots,n\}$, the eigenvalues of $T_j$ and $T_{j-1}$ interlace as follows:
 	$$\lambda_j(T_j) <\lambda_{j-1}(T_{j-1}) < \lambda_{j-1}(T_j) <\cdots< \lambda_2(T_j) <\lambda_1(T_{j-1}) < \lambda_1(T_j)$$.
 \end{theorem}
 
 This means that we can construct a hierarchy of the zeros of these polynomials and use information about all of their signs to tell how many zeros fall below a given value. \autoref{fig:charpoly-zero} nicely illustrates this structure and leads us to a couple of observations. Let's denote the number of sign flips in the sequence of polynomilas (evaluated at a certain $x$) $p_n(x), p_{n-1}(x), \dots, p_1(x), p_0(x)$ by $s(x)$ . 
 \begin{enumerate}
 	\item At the negative side as $x \to -\infty$, we see that since the leading term of the $j^{\mathrm{th}}$ polynomial is $\left(-1\right)^j x_j$, therefore $p_j \to \infty$, and hence the sign will be positive for all polynomials at a sufficiently low $x$, which results in $s(x)\to0$. On the flip side, for sufficiently large $x$ however $s(x)$ as the signs will alternate due to the $\left(-1\right)^j$ term.
 	\item Coming from negative $x$ values, between two eigenvalues, i.e. zeros of $p_n$, the number sign flips cannot change. This is due to the propery of interlaced zeros: if $p_j(x)$ flips in sign, then $p_{j+1}$ must have had a zero before that which flipped it, i.e. it will have the same sign as $p_j(x)$.
 	\item This leads to the last conclusion: $p_n$ introduces sign flips (because there is no $p_{n+1}$) at each of it's zeros, which are the eigenvalues.
 \end{enumerate}
 
 Thereby we can conclude that $s(x)$ describes the number of eigenvalues of $T$, which are smaller than $x$.
 
 \begin{figure}
 	\centering
 	\begin{tikzpicture}[line width=3pt]
 		% Define the list of lists
 		\def\listOfLists{{},{5},{3,6},{1.5,3.5,8},{1.3,2.5,4,8.5},{0.7,1.8,3,7,8.7},{0.3,1.4,2,4.5,8.4,9}}
 		
 		% Define the colors for alternating connections
 		\definecolor{color1}{RGB}{0, 0, 255}
 		\definecolor{color2}{RGB}{255, 0, 0}
 		\colorlet{linecolor}{color1}
 		% Define the vertical spacing between levels
 		\def\verticalSpacing{-0.65}
 		
 		% Initialize the y-coordinate
 		\pgfmathsetmacro{\ycoord}{0}
 		
 		% Loop through the list of lists
 		\newcounter{arraycard}
 		\foreach \level [count=\i] in \listOfLists {
 			% Get the length of the current \level
 			\setcounter{arraycard}{0}%
 			% Draw nodes for each element in the sublist
 			\foreach \x [count=\j] in \level {
 				\pgfmathparse{\ycoord + (\i - 1) * \verticalSpacing}
 				
 				\node[mark=square, draw, fill=black, inner sep=2pt] (node\i\j) at ($(\x, \pgfmathresult)$) {};
 			}
 			
 			% Connect nodes on the same level with alternating colors
 			
 			\foreach \x [count=\j] in \level {
 				\let\ycoord\pgfmathresult
 				\ifodd\j
 				\colorlet{linecolor}{color2}
 				\else
 				\colorlet{linecolor}{color1}
 				\fi
 				\ifnum\j>1
 				\draw[linecolor] (node\i\j) -- (node\i\the\numexpr\j-1\relax);
 				\fi
 				\stepcounter{arraycard}%
 			}
 			% Use \levelLength to access the length of the \level
 			%\node[above right] at (node\i\the\value{arraycard}) {\the\value{arraycard}\ elements};
 			\pgfmathparse{int(\i-1)}
 			\node[draw=none] (node\i0) at ($(0, \ycoord + \i* \verticalSpacing - 1 * \verticalSpacing)$) {};
 			\node (name\i) at ($(node\i0)+(-0.5,0)$) {$p_\pgfmathresult$};
 			\pgfmathparse{int(\the\value{arraycard}+1)}
 			\node (node\i\pgfmathresult) at ($(10, \ycoord + \i* \verticalSpacing - 1 * \verticalSpacing)$) {};
 			\draw[color2] (node\i1) --($(0, \ycoord + \i* \verticalSpacing - 1 * \verticalSpacing)$);
 			
 			\ifodd\the\value{arraycard}
 			\colorlet{linecolor}{color1}
 			\else
 			\colorlet{linecolor}{color2}
 			\fi
 			\pgfmathparse{int(\the\value{arraycard}+1)}
 			\draw[linecolor,->] (node\i\the\value{arraycard}) -- (node\i\pgfmathresult);
 		}
 	\end{tikzpicture}
 	
 	\caption{
 		Visualizing zeros of the different levels of characteristic polynomials and their interlaced zeros. The lines represent the real $x$ axis, indicating where the value of the polynomial is {\color{red}positive (red)} or {\color{blue}negative (blue)} or zero ($\blacksquare$)
 	}\label{fig:charpoly-zero}
 \end{figure}
\FloatBarrier
\newpage
\printbibliography
\end{document}