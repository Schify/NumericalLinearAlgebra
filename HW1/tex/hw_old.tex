\documentclass{article}
\usepackage[top=2cm, bottom=2cm, left=2.2cm, right=2.5cm]{geometry}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{enumerate}% http://ctan.org/pkg/enumerate

\usepackage[]{pdfcomment}
%As recommended by matlab2tikz
\usepackage{pgfplots}
\usepackage{tikz}
\usepackage{nicematrix}
\usepackage{calc}
\usepackage{pgfplotstable}
\usepackage{booktabs}
\usepackage{csvsimple}
\usepackage{etoolbox}
\usepackage[style=alphabetic]{biblatex}
\usepackage{amsthm}
\bibliography{refs.bib}
%% the following commands are needed for some matlab2tikz features
\usetikzlibrary{plotmarks}
\usetikzlibrary{arrows.meta}
\usepgfplotslibrary{patchplots}
\usepackage{grffile}
%% you may also want the following commands
\pgfplotsset{plot coordinates/math parser=false}
%\newlength\figureheight
%\newlength\figurewidth
\usepackage{hyperref}
\usepackage{placeins,nicefrac}
\usepackage{xstring}
\usepackage[]{pgfkeys}
\usepackage{xfrac}
\usepackage{minted}
\usetikzlibrary{external}
\usetikzlibrary{calc,math}
\tikzexternalize[prefix=extern/]
\usepgfplotslibrary{groupplots}
\usepgfplotslibrary{colormaps}
\usetikzlibrary{decorations.markings,patterns,patterns.meta,pgfplots.fillbetween}

\usepackage[ruled,vlined,linesnumbered,algo2e]{algorithm2e}
\usepackage{wrapfig}
\usepackage{subcaption}
\graphicspath{../plots/}

\usepackage{wasysym}
\usepackage{animate}

%\usepackage{expl3}
%\ExplSyntaxOn
%\cs_set_eq:NN \fpeval \fp_eval:n
%\ExplSyntaxOff

\usetikzlibrary{fpu}
\graphicspath{{../figs/}}
\newcommand{\figurescale}{1.0}

%\usepackage{titlesec}
%\titleformat{\subsection}{\normalfont\large\bfseries}{Task \thesubsection}{1em}{}
%\titleformat{\section}{\normalfont\Large\bfseries}{Assignment \thesection}{1em}{}
%\titleformat{\subsubsection}{\normalfont\bfseries}{Question \thesubsubsection}{1em}{}
\newcommand\mycommfont[1]{\ttfamily\textcolor{blue}{#1}}
\SetCommentSty{mycommfont}

\newcommand{\stilltodo}[1]{{\color{red} UNFINISHED #1}}

\makeatletter
\newcommand{\trp}{%
	{\mathpalette\@transpose{}}%
}
\newcommand*{\@transpose}[2]{%
	% #1: math style
	% #2: unused
	\raisebox{\depth}{$\m@th#1\intercal$}%
}

\makeatother
\newcommand{\diff}{\mathrm{d}}
\author{Andr\'as Schifferer, r0915705}
\title{Numerical Linear Algebra [H03G1A]\\{\LARGE HW 1}}
\date{\today} 

\pgfplotsset{plot coordinates/math parser=false}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}


\begin{document}
	\maketitle
	%The assignment is not yet finished unfortunately.
	\tableofcontents
	
	
	
	\newpage
	\section{Semi-Orthogonality and strategic reorthogonalization}
	\textit{Note that throughout the first part of the assignment if an algorithm is performed on an unspecified matrix, then the subjected matrix should be understood to be the one provided for the assignment as \texttt{Test.mtx}.
	}
	
	The LÃ¡nczos algorithm can be used to construct a Hessenberg decomposition of a matrix plus a rank 1 term. If we restrict the matrix inputs to real symmetric matrices, then the symmetry requirement will force the apprximately similar Hessenberg matrix to be symmetric as well, which results necessarily in a tridiagonal matrix.
	\begin{algorithm2e}[ht]
		
		\SetKwInOut{Input}{input}
		\SetKwInOut{Output}{output}
		\SetKw{Init}{init}{}{}
		\SetAlgoLined
		\Input{Linear, symmetric real operator $A$ on $\mathbb{R}$}
		\Output{Approximately orthogonally similar tridiagonal $T\sim A$, given by diagonals $\mathbf{\alpha},\mathbf{\beta}$}
		\Init{choose starting vector $\mathbf{r}_0$,$\beta_0:=\|\mathbf{r}_0\|$, $\mathbf{q}_0=0$}\\
		\For{$j=1,2,...$}{
			$\mathbf{q}_j:=\mathbf{r}_{j-1}/\beta_{j-1}$\\
			$\mathbf{v}_j=A\mathbf{q}_j-\beta_{j-1}\mathbf{q}_{j-1}$\\
			$\alpha_j=\mathbf{v}_j^{\ast}\mathbf{q}_j$\\
			$\mathbf{r}_j=\mathbf{v}_j-\alpha_j\mathbf{q}_j$\\
			$\beta_j=\|\mathbf{r}_j\|$\\
		}
		\caption{Lanczos in exact arithmetic}\label{alg:Lanczos}
	\end{algorithm2e}\\
	
	The matrix $T$ then has the form 
	$$T=\begin{pmatrix}
		\alpha_1&\beta_1&0&\cdots&0\\
		\beta_1&\alpha_2&\beta_2&&0\\
		\vdots&\ddots&\ddots&\ddots&\vdots\\
		0&&\beta_{j-2}&\alpha_{j-1}&\beta_{j-1}\\
		0&\cdots&0&\beta_{j-1}&\alpha_{j}\\
	\end{pmatrix}$$
	One step in the main loop of \autoref{alg:Lanczos} is called a Lanczos step, and is usually written as
	\begin{equation}\label{eq:LanczosStep}
		\beta_j\mathbf{q}_{j+1}=A\mathbf{q}_j-\alpha_j\mathbf{q}_j-\beta_{j-1}\mathbf{q}_{j-1}.
	\end{equation}
%\part{title}	\inputminted[firstline=2, lastline=5]{matab}{../NLAHW1_Lanczos/test.m}
	
	We know that, with $Q_{j}:=[\mathbf{q}_1,\ldots,\mathbf{q}_j]$, we have the relation
	\begin{equation}\label{eq:matLanczos}
		AQ_{j}=Q_jT_j+\beta_{j}\mathbf{q}_{j+1}\mathbf{e}^{\ast}.
	\end{equation}
	In exact arithmetic the above is a great procedure for tridiagonalization. Numerically this is
	not the case unfortunately, as the orthogonality of $Q$ gets diminished by numerical errors.
	
	\subsection{Lanczos0: 'theoretical' Lanczos}
	
	The theoretical Lanczos algorithm (hereby reffered to as Lanczos0) will lose stability after only a small number of iterations, as examplified in \autoref{fig:lanczos0_W}
	\begin{figure}
		\centering
		\resizebox{\textwidth}{!}{
		\input{../plots/lanczos0_W.tex}}
		\caption{Loss of orthogonality in Lanczos 0}\label{fig:lanczos0_W}
	\end{figure}
	We can see that orthogonality is rapidlly lost at about iteration 20, and after that point a significant part of the former vectors are no longer orthogonal to the new ones.
	
	This can be further corrobarated by the convergence of the Ritz values as seen in \autoref{fig:lanczos0_W_ritz}. Note that the test matrix (provided in \texttt{Test.mtx}) only has real eigenvalues more than or equal to one. We can see that in 
	comparison with the better performing Lanczos1 algorithm (to be discussed later) where the Ritz values seem to be doing a good job approximating the spectrum of eigenvalues (in \autoref{fig:lanczos1_W_ritz}). In contrast Lanczos0 has trouble fully covering the eigenvalues in the Test matrix. Even at higher iteration numbers (i.e. a larger number of Ritz values) sometimes a spurious Ritz value appears, for example between the two largest eigenvalues at $j=39$. This might be explained by the last vector having a too high component parallel to the eigenvector corresponding to a larger eigenvalue. Therefore this might change the eigenvalues of $T_k$, which results in a bad quality approximation of the spectrum.
	
	
	
	\subsection{Lanczos1: 'practical' Lanczos}
	% It is not practical because calculating Q*Q is a lot of effort
	
	\begin{algorithm2e}[ht]
		\SetAlgoNlRelativeSize{-1}
		\caption{Reorthogonalization against all previous $\textbf{q}_i$}\label{alg:reorthogonalization}
		\KwIn{$Q_k$: Matrix with quasi-orthonormal columns}
		\KwIn{$\textbf{r}$: Vector to be reorthogonalized}
		\KwOut{$\textbf{r}_{\text{new}}$: Reorthogonalized vector}
		
		$\textbf{r} \gets \textbf{r} - Q_{k-1} Q_{k-1}^\ast \textbf{r}$\\
		$\alpha_{\text{loc}} \gets \textbf{q}_k^\ast \cdot r$\\
		$\textbf{r}_{\text{new}} \gets \textbf{r} - \alpha_{\text{loc}}  \textbf{q}_k$\\
	\end{algorithm2e}
	
	\begin{figure}
		\centering
		\resizebox{\textwidth}{!}{
			\input{../plots/lanczos1_W.tex}}
		\caption{Loss of orthogonality in Lanczos 1. Note that $w_{k,\infty}$ is always shown, as the one used in testing for reorthogonalization (i.e. the projected one in the next step)}\label{fig:lanczos1_W}
	\end{figure}
	
	\begin{figure}
	\centering
	\resizebox{0.95\textwidth}{!}{
		\includegraphics[trim=0 180 0 250 clip]{../plots/lanczos0_ritz.pdf}}
	\caption{Lanczos0: The eigenvalues of the test matrix ($\lambda_i$'s histogram) and the Ritz values at the $j^{\mathrm{th}}$ iteration ($\mu_i$, denoted by ${\color{red} \times}$ )}\label{fig:lanczos0_W_ritz}
	\end{figure}
	
	\begin{figure}
	\centering
	\resizebox{0.95\textwidth}{!}{
		\includegraphics[trim=0 170 0 180,clip]{../plots/lanczos1_ritz.pdf}}
	\caption{Lanczos1: The eigenvalues of the test matrix ($\lambda_i$'s histogram) and the Ritz values at the $j^{\mathrm{th}}$ iteration ($\mu_i$, denoted by ${\color{red} \times}$ )}\label{fig:lanczos1_W_ritz}
	\end{figure}
	
	\FloatBarrier
	
	\subsection{Lanczos2: heuristic for loss of orthogonality}
	
	Calculating $W = Q^\ast Q$ was the most costly part of the Lanczos1 algorithm, especially since this step needs to be done at each iteration, to determine whether a correction is needed or not in the first place. Loss of orthogonality could be predicted using a parallel system for iteratively constructing $\tilde{W}$, which could serve as a proxy to gauge when loss of orthogonality is to be expected. Our main assumption going forward will be that floating point errors have comaprable behaviour to that of Gaussian noise.
	
	Ideally in exact arithmetic $W_j = \mathbb{I}_{j\times j}$ (identity) would hold. However if at each iteration, i.e. the calculation of the new $\textbf{q}_j$, we could have access to the exact arithmetic error (which is assumed to be random and therefore inaccessible), we could predict the evolution of the non-ideal $W_j$. First let's look at the Lanczos step in \autoref{eq:LanczosStep}. By adding the floating point error $\textbf{f}_j$
	the equation becomes
	\begin{equation}\label{eq:LanczosStep-f}
		\beta_j\mathbf{q}_{j+1}=A\mathbf{q}_j-\alpha_j\mathbf{q}_j-\beta_{j-1}\mathbf{q}_{j-1} - \textbf{f}_j.
	\end{equation}
	This can be transformed in a similar fashion as before into 
	\begin{equation}\label{eq:matLanczos-F}
		AQ_{j}=Q_jT_j+\beta_{j}\mathbf{q}_{j+1}\mathbf{e}^{\ast} + F_j,
	\end{equation}
	where $F_j$ is a matrix collecting the error terms.
	
	We will assume, that at any given point the bases has remained sufficiently orthonormal, and therefore 
	\begin{align}
		\left\lVert \textbf{q}_k \right\rVert_2 &= 1,\\
		\beta_{k} \textbf{q}_k^\ast \textbf{q}_{k+1} &= C \epsilon \left\lVert A \right\rVert,
	\end{align}
	where $C$ is a modest constant. This presupposes the fact that we won't let the orthogonality to diminish too much.
	
	We can manipulate \autoref{eq:LanczosStep-f} by taking the inner product with $\textbf{q}_k$. Thereby
	\begin{equation}\label{eq:reccurence-mult}
		\beta_j\mathbf{q}_k^*\mathbf{q}_{j+1}=\mathbf{q}_k^*A\mathbf{q}_j-\alpha_j\mathbf{q}_k^*\mathbf{q}_j-\beta_{j-1}\mathbf{q}_k^*\mathbf{q}_{j-1} - \mathbf{q}_k^*\textbf{f}_j,
	\end{equation}
	holding true for $j=2,3,\dots J$ and $k=1,2, \dots J$. If we again define $W_J = Q_J^\ast Q_J$, and we denote the elements of this matrix as $w_{x,y}$, then a number of identities may follow, such as 
	\begin{align}
		w_{k,k} &=  1, \quad&&\mathrm{for}\,\, k = 1,2 \dots J\label{eq:wconds1}, \\
		w_{k,k-1} &= \underbrace{\textbf{q}_k^\ast \textbf{q}_{k-1}}_{\psi_k := } \quad&&\mathrm{for}\,\, k = 2,3 \dots J\label{eq:wconds2}, \\
		w_{j,k+1} &=  w_{k+1, j}\quad&&\mathrm{for}\,\, k = 1,2 \dots J-1\label{eq:wconds3}.
	\end{align}
	\autoref{eq:wconds1} is just a result of our stipulation that the system is orthonormal, and we are able to sufficiently normalize the vectors. \autoref{eq:wconds3} follows from the symmetry of $W_{J}$, which is inherent from its definition, as well as \autoref{eq:wconds2} evidently.
	
	Notice, that in \autoref{eq:reccurence-mult} the indicies can be swaped, so taking \autoref{eq:LanczosStep-f} at $k$ and multiplying it by $\textbf{q}_{j}$ we get
	\begin{equation}\label{eq:reccurence-mult-k}
		\beta_k\mathbf{q}_j^*\mathbf{q}_{k+1}=\mathbf{q}_j^*A\mathbf{q}_k-\alpha_k\mathbf{q}_j^*\mathbf{q}_k-\beta_{k-1}\mathbf{q}_j^*\mathbf{q}_{k-1} - \mathbf{q}_j^*\textbf{f}_k.
	\end{equation}
	Looking at the difference of \autoref{eq:reccurence-mult} and \autoref{eq:reccurence-mult-k}
	\begin{equation}
		\beta_j\mathbf{q}_k^*\mathbf{q}_{j+1} - \beta_k\mathbf{q}_j^*\mathbf{q}_{k+1}=\underbrace{\mathbf{q}_k^*A\mathbf{q}_j - \mathbf{q}_j^*A\mathbf{q}_k}_{=0} +
		\alpha_k\mathbf{q}_j^*\mathbf{q}_k - \alpha_j\mathbf{q}_k^*\mathbf{q}_j + \beta_{k-1}\mathbf{q}_j^*\mathbf{q}_{k-1} - \beta_{j-1}\mathbf{q}_k^*\mathbf{q}_{j-1} + \mathbf{q}_j^*\textbf{f}_k  - \mathbf{q}_k^*\textbf{f}_j,
	\end{equation}
	where $\mathbf{q}_k^*A\mathbf{q}_j - \mathbf{q}_j^*A\mathbf{q}_k = 0$ thanks to the symmetry of $A$. Thereby
	\begin{equation}\label{eq:w-update}
		\beta_j w_{k,j+1} = 
		\beta_k w_{j,k+1} + \left(\alpha_k - \alpha_j\right) w_{k,j} + \beta_{k-1} w_{j, k-1} - \beta_{j-1} w_{k,j-1} + \underbrace{\mathbf{q}_j^*\textbf{f}_k-\mathbf{q}_k^*\textbf{f}_j}_{\theta_{j,k} 
		:=}.
	\end{equation}
	 
	 This update law of the $W_J$ matrix allows us to iteratively construct the matrix for $k=1,\dots j-1$ (stipulating that $w_{k,0} = 0$). Here we can assume, that $\psi_k$ and $\theta_{j,k}$ are just noise introduced by imperfect arithmetics, and as such can be stochastically modelled.
	 
	 The algorithm can be losely descired as the following:
	 \begin{itemize}
	 	\item At any given moment the last two rows of $W_k$ are stored.
	 	\item These two rows are used to calculate the next row in $W_j$ by iterating outwards from the first subdiagonal using \autoref{eq:w-update}
	 	\begin{itemize}
	 		\item the first subdiagonals contain only errors from floating point arithmetic so they are drawn as $\psi_{k}=\sqrt{n}\epsilon(\beta_{1}/\beta_{j}) z_2$,
	 		\item the arithmetic error from the update using iteration is also random $\theta_{jk}=\epsilon(\beta_{j}+\beta_{k})z_1$ with $z_1\sim \mathcal{N}(0,.3)$.
	 	\end{itemize}
	 	\item If the new row indicates that a reorthogonalization is needed, we reorthogonalize both in this and the subsequent iteration step. Since this means that the elements of this row should be almost zero, we reroll the relevant offdiagonal values of the current $w_{*,j+1}$ row to $\epsilon z_3$, with $z_3\sim\mathcal{N}(0,3/2)$.
	 \end{itemize}
	 
%	 You can take $\theta_{jk}=\epsilon(\beta_{j}+\beta_{k})z_1$ with $z_1\sim \mathcal{N}(0,.3)$ and $\psi_{k}=\sqrt{n}\epsilon(\beta_{1}/\beta_{j}) z_2$ with $z_2\sim \mathcal{N}(0,.6)$. You can update the selected elements in $w$ to $\epsilon z_3$, with $z_3\sim\mathcal{N}(0,3/2)$. You do not need to worry where these numbers come from.\\

\begin{algorithm2e}[ht]
	
	\SetKwInOut{Input}{input}
	\SetKwInOut{Output}{output}
	\SetKw{Init}{init}{}{}
	\SetAlgoLined
	\Input{Linear, symmetric real operator $A$ on $\mathbb{R}$}
	\Output{Approximately orthogonally similar tridiagonal $T\sim A$, given by diagonals $\mathbf{\alpha},\mathbf{\beta}$}
	\Init{choose starting vector $\mathbf{r}_0$,$\beta_0:=\|\mathbf{r}_0\|$, $\mathbf{q}_0=0$, $W_0=\begin{bmatrix} 0 \end{bmatrix}$}\\
	\For{$j=1,2,...$}{
		$\mathbf{q}_j:=\mathbf{r}_{j-1}/\beta_{j-1}$\\
		$\mathbf{v}_j=A\mathbf{q}_j-\beta_{j-1}\mathbf{q}_{j-1}$\\
		$\alpha_j=\mathbf{v}_j^{\ast}\mathbf{q}_j$\\
		$\mathbf{r}_j=\mathbf{v}_j-\alpha_j\mathbf{q}_j$\\
		$\beta_j=\|\mathbf{r}_j\|$\\
		\If{$j<k_{\text{max}}-1$ \textbf{and} $j>1$}{
			$W_{j}, W_{j-1} \leftarrow \text{update\_w}(W_{j-1}, W_{j-2}, \alpha, \beta, n)$ \tcp{Updating $W$-prox with \autoref{alg:update_w}}
			$\omega_{j,\infty} \leftarrow \max_{1\leq k \leq j-1}(\lvert W_j(k) \rvert)$\\
			\If{$\omega_{j,\infty} > \delta$}{
				\tcp{Reorthogonalization needed}
				$\mathbf{r}_j, \beta_j \leftarrow \text{reorthogonalization}(Q_k(:, 1:j), \mathbf{r}_j, \beta_j)$ \tcp{Reorthogonalize}
				$W_j(1:end-1) \leftarrow \epsilon z_3$ \tcp{Reroll off-diagonal values}
			}
		}
	}
	\caption{Lanczos2 with practically detected reorthogonalization (only one reorth for simplicity, in practice this migth differ)}\label{alg:LanczosQuasiOrtho}
\end{algorithm2e}

\begin{algorithm2e}[ht]
	\SetKwInOut{Input}{input}
	\SetKwInOut{Output}{output}
	\Input{$w$: Current column vector of $W_j$}
	\Input{$w_{\text{old}}$: Previous column vector of $W_{j-1}$}
	\Input{$\alpha$: Vector of Lanczos $\alpha$ values}
	\Input{$\beta$: Vector of Lanczos $\beta$ values}
	\Input{$n$: Size of the matrix}
	\Output{$w$: Column vector of $W_{j+1}$}
	\Output{$w_{\text{old}}$: Updated column vector of $W_{j}$}
	
	\tcp{Add initial zeros for $k=0$}
	$j \leftarrow$ size($w$, 1); 
	$w \leftarrow$ [0; $w$]; 
	$w_{\text{old}} \leftarrow$ [0; $w_{\text{old}}$]; 
	$\beta \leftarrow$ [0; $\beta$]; 
	$\alpha \leftarrow$ [0; $\alpha$]\\
	$w_{\text{new}} \leftarrow {0}_{j+2}$ \tcp{Allocation}
	
	\tcp{The diagonal and off-diagonal elements in $W$}
	$w_{\text{new}}(j+2) \leftarrow 1$ \tcp{Diagonal}
	$w_{\text{new}}(j+1) \leftarrow \sqrt{n}\epsilon\beta(2)/\beta(j+1) \times (0.6 \times \text{randn}())$ \tcp{$\psi_{j}$ on the off-diagonal}
	
	\For{$k \leftarrow j$ \textbf{downto} $2$}{
		\tcp{Because of the extension of the vectors, the loop runs a bit differently}
		\tcp{$k$ here corresponds to $k+1$ in the report}
		$\theta_{j,k} \leftarrow \epsilon(\beta(j+1)+\beta(k)) \times 0.3 \times \text{randn}()$ \tcp{$\theta_{j,k}$}
		
		$w_{\text{new}}(k) \leftarrow (\beta(k)w(k+1) + (\alpha(k)-\alpha(j+1))w(k) + \beta(k-1)w(k-1) - \beta(j)w_{\text{old}}(k) + \theta_{j,k})/\beta(j+1)$
	}
	
	\tcp{Drop off the virtual zeros}
	$w_{\text{old}} \leftarrow w(2:$end$)$\\
	$w \leftarrow w_{\text{new}}(2:$end$)$
	
	\caption{Update $W_j$ (using more \texttt{Matlab} heavy notation, \texttt{randn()}$\sim \mathcal{N}(0,1)$)}\label{alg:update_w}
\end{algorithm2e}


\subsection{The unavoidability of orthogonality loss}
\textit{Similar analysis has been performed by \textcite{paige-1980}, and some of the steps might be losely followed, as the assignment's formulas are proven.}

To investigate the unavoidability of orthogonality loss let's look a bit deeper into the accumulation of the floating point arithmetic error. Suppose that $W_j:=I_j+U_j+U_j^{\ast}$ with $U_j$ strictly upper triangular. Suppose the columns of $U$ are given by $U=[\mathbf{u}_1,\ldots,\mathbf{u}_j]$.
Then we can simply use the definition of $W_j$ to see that
\begin{equation}
	W_j = Q_j^\ast Q_j = Q_j^\ast \begin{bmatrix}
		{\bf q}_1 &{\bf q}_2 &\dots &{\bf q}_j
	\end{bmatrix} = \begin{bmatrix}
	Q_j^\ast {\bf q}_1 &Q_j^\ast {\bf q}_2 &\dots &Q_j^\ast {\bf q}_j
	\end{bmatrix}.
\end{equation}
Looking at the last column of this expression we can see that 
\begin{equation}
	Q_j^\ast {\bf q}_j = {\bf e}_j+ {\bf u}_j
\end{equation}
since the last column of $W_j$ has no component of the strictly lower triangular $U_j^\ast$. Similarly by looking at the second to last column of $W_j$ we see that 
\begin{equation}
	Q_j^\ast {\bf q}_{j-1} = {\bf e}_{j-1}+ {\bf u}_{j-1} + \begin{bmatrix}
		0 & 0 & \dots & w_{j,j-1}
	\end{bmatrix}^\intercal = {\bf e}_{j-1}+ {\bf u}_{j-1} + {\bf q}_{j-1}^\ast {\bf q}_j {\bf e}_j
\end{equation}
using the definition of the elements in $W_j$.

Now take \autoref{eq:LanczosStep-f} and multiply it by $Q_j^\ast$ from the left so that we get 
\begin{equation}\label{eq:lanc-step-fqj}
	\beta_jQ_j^\ast\mathbf{q}_{j+1}=Q_j^\ast A\mathbf{q}_j-\alpha_jQ_j^\ast\mathbf{q}_j-\beta_{j-1}Q_j^\ast\mathbf{q}_{j-1} - Q_j^\ast\textbf{f}_j.
\end{equation}
In order to eliminate the $Q_j^\ast A\mathbf{q}_j$ term take \autoref{eq:matLanczos-F}'s (conjugate-)transpose and multiply it with $\textbf{q}_j$ fromt he right:
\begin{equation}
	Q_{j}^\ast A^\ast \textbf{q}_j  =T_j^\ast Q_j^\ast \textbf{q}_j +\beta_{j}\mathbf{e}_j\mathbf{q}_{j+1}^{\ast}\textbf{q}_j + F_j^\ast \textbf{q}_j
\end{equation} 

Since we assumed $A$ to be symmetic and real, and thereby also $T_k$, we can substitute into \autoref{eq:lanc-step-fqj} to obtain
\begin{equation*}
	\beta_jQ_j^\ast\mathbf{q}_{j+1}=T_j \left({\bf e}_j+ {\bf u}_j\right) +\beta_{j}\mathbf{e}\mathbf{q}_{j+1}^{\ast}\textbf{q}_j + F_j^\ast \textbf{q}_j-\alpha_j \left({\bf e}_j+ {\bf u}_j\right)-\beta_{j-1}\left({\bf e}_{j-1}+ {\bf u}_{j-1} + {\bf q}_{j-1}^\ast {\bf q}_j {\bf e}_j\right) - Q_j^\ast\textbf{f}_j.
\end{equation*}
Fortunately just from the definition of $T_j$ we can know that $T_j \textbf{e}_j = \alpha_{j} \textbf{e}_j + \beta_{j-1} \textbf{e}_{j-1}$, therefore we arrive at
\begin{equation}\label{eq:error-accum}
	\beta_jQ_{j}^{\ast}\mathbf{q}_{j+1}=T_{j}\mathbf{u}_j-\alpha_j\mathbf{u}_j-\beta_{j-1}\mathbf{u}_{j-1}+\underbrace{(F_j^{\ast}\mathbf{q_j}-Q_j^{\ast}\mathbf{f}_j)}_{\mathbf{g}_j:=}+\mathbf{e}_j(\beta_{j}\mathbf{q}_{j+1}^{\ast}\mathbf{q}_{j}-\beta_{j-1}\mathbf{q}_{j}^{\ast}\mathbf{q}_{j-1})
\end{equation}

This above relation comes from the L\'anczos step ultimately, and therefore could apply for any index up to $j$, but of course some dimensionalities would change in that case. Let's take $k<j$ (here of course $\textbf{u}_k \in \mathbb{R}^k$ or $\mathbb{C}^k$):
\begin{equation*}
	\beta_kQ_{k}^{\ast}\mathbf{q}_{k+1}=T_{k}\mathbf{u}_k-\alpha_k\mathbf{u}_k-\beta_{k-1}\mathbf{u}_{k-1}+(F_k^{\ast}\mathbf{q_k}-Q_k^{\ast}\mathbf{f}_k)+\mathbf{e}_k(\beta_{k}\mathbf{q}_{k+1}^{\ast}\mathbf{q}_{k}-\beta_{k-1}\mathbf{q}_{k}^{\ast}\mathbf{q}_{k-1})
\end{equation*}
where of course, due to the definition of $U_k$, $Q_{k}^{\ast}\mathbf{q}_{k+1} = \textbf{u}_{k+1}$. Since this is an upper triangular we can always append $j-k$ zeros below the vectors in the equation, so that the dimension agrees with \autoref{eq:error-accum}, and thereby the extended $u_{k+1} $ corresponds to a column in $U_j$. After this extension for example $T_{k}\mathbf{u}_k$ becomes $T_{j}\mathbf{u}_k$, where the second $u_k$ is of dimension $j$, in contrast with the first $u_k$. The result of these two expressions also only differs in the number of end zeros, which ensures further consistency of our manipulations. Therefore now we will use this extended equation as 
\begin{equation}\label{eq:inbetween-error-accum}
	0 =T_{j}\mathbf{u}_k\underbrace{-\overbrace{\beta_k \mathbf{u}_{k+1}}^{Q_{k}^{\ast}\mathbf{q}_{k+1}}-\alpha_k\mathbf{u}_k-\beta_{k-1}\mathbf{u}_{k-1}}_{=-U_j T_j \textbf{e}_k}+\underbrace{(F_k^{\ast}\mathbf{q_k}-Q_k^{\ast}\mathbf{f}_k)+\mathbf{e}_k(\beta_{k}\mathbf{q}_{k+1}^{\ast}\mathbf{q}_{k}-\beta_{k-1}\mathbf{q}_{k}^{\ast}\mathbf{q}_{k-1})}_{\tilde{\textbf{g}}_k := }
\end{equation}
with $\tilde{\textbf{g}}_k$ extending not just by the padding zeros to conform to the new dimensions, but also including the numerical errors that remain after orthogonalizing consecutive vectors as relating to the operations ($\beta_{k-1}\textbf{q}_k^\ast \textbf{q}_{k-1}$ and such)

We can collect \autoref{eq:inbetween-error-accum} valid for $1 \leq k < j$, and \autoref{eq:inbetween-error-accum}, and stack them column by column into matrices. Notice how the left hand side will collect the end error that results from the inperfect approximation of the matrix. This comes from the $j^{\mathrm{th}}$ step, and for all other purposes the equations are the same.
This means that 
\begin{equation*}
	\begin{bmatrix}
		0 & 0 & \dots & \beta_kQ_{k}^{\ast}\mathbf{q}_{k+1}
	\end{bmatrix} 
	= 
	T_j \underbrace{\begin{bmatrix}
		\textbf{u}_1 &
		\textbf{u}_2 &
		\dots &
		\textbf{u}_j
	\end{bmatrix}}_{U_j}
	- U_j T_j \underbrace{\begin{bmatrix}
		\textbf{e}_1 &
		\textbf{e}_2 &
		\dots &
		\textbf{e}_j
	\end{bmatrix}}_{I_j}
	+ \underbrace{
	\begin{bmatrix}
		\tilde{\textbf{g}}_1 &
		\tilde{\textbf{g}}_2 &
		\dots &
		\tilde{\textbf{g}}_j
	\end{bmatrix}
	}_{G_j:=}
\end{equation*}
which results in the compact formula as follows:
\begin{equation}\label{eq:full-orth-loss}
	\beta_j Q_j^\ast \textbf{q}_{j+1} \textbf{e}_j^\ast = T_j U_j - U_j T_j + G_j 
\end{equation}

Now we'll go on to prove Paige's lemma.
\begin{lemma}[Paige, 1971, see more in \autocite{paige-1980}]\label{lemma:paige}
	Let $T_j$ denote the Lanczos matrix for some given A at iteration j. Suppose $T_j$ has an eigendecomposition $T_jS_j=S_j\Theta_j$, with associated Ritz vectors $Y=Q_jS_j=Q_j[\mathbf{s}_1,\dots,\mathbf{s}_j]$. Say $s_{ji}=\mathbf{e}_j^{\ast}\mathbf{s}_i$. Let
	$(\theta_i , \textbf{y}_i)$ be a Ritz pair at this iteration for $T_j$. Define 
	\begin{equation}\label{eq:betaij}
		\beta_{ji} := \|A\textbf{y}_i - \theta_i \textbf{y}_i\|_2=\beta_j|s_{ji}|
	\end{equation}
	Then
	\begin{equation}\label{eq:eig-orth-loss}
		|\textbf{y}_i^{\ast}\textbf{q}_{j+1}| =\frac{|\gamma_{ii}|}{\beta_{ji}}
	\end{equation}
	with $\gamma_{ii}=\mathbf{s}_i^{\ast}G_{j}\mathbf{s}_i\approx\epsilon\|A\|$.
\end{lemma}
\begin{proof}
	By using the definition of the eigenvectors of $T_j$ and \autoref{eq:matLanczos} (so thereby ignoring the floating point operations' error)
	\begin{equation*}
		A \textbf{y}_i - \theta_i \textbf{y}_i = A Q_j \textbf{s}_i - Q_j \theta_i s_i = \left(A Q_j - Q_j T_j  \right) \textbf{s}_i = \beta_j \textbf{q}_{j+1} \textbf{e}_j^\ast \textbf{s}_i
	\end{equation*}
	Taking the norm we see that \autoref{eq:betaij} holds true as
	\begin{equation*}
		\left\lVert A \textbf{y}_i - \theta_i \textbf{y}_i \right\rVert =\left\lVert \beta_{j} \textbf{q}_{j+1} \textbf{e}_j^\ast \textbf{s}_i \right\rVert =\left\lVert \textbf{q}_{j+1} \textbf{y}_i \right\rVert  \beta_{j} \left|s_{i,j}\right| = \beta_{j} \left|s_{i,j}\right| =: \beta_{ji}
	\end{equation*}
	thanks to $\beta_j\geq 0$ (due to it being defined as the norm of a vector), and $q_{j+1}$ being a normalized.
	
	Next we can reformulate \autoref{eq:full-orth-loss} by multiplying it with $\textbf{s}_i^\ast$ from the left and $\textbf{s}_i$ from the right
	\begin{equation*}
		\beta_j \underbrace{\textbf{s}_i^\ast Q_j^\ast}_{\textbf{y}_i^\ast} \textbf{q}_{j+1} \underbrace{\textbf{e}_j^\ast\textbf{s}_i}_{s_{ji}} = \underbrace{\textbf{s}_i^\ast T_j U_j \textbf{s}_i - \textbf{s}_i^\ast U_j T_j \textbf{s}_i}_{=0} + \underbrace{\textbf{s}_i^\ast G_j \textbf{s}_i}_{\gamma_{ii}},
	\end{equation*}
	where again from the definition of the eigenvectors $\textbf{s}_i^\ast T_j U_j \textbf{s}_i - \textbf{s}_i^\ast U_j T_j \textbf{s}_i=\theta_i \textbf{s}_i^\ast U_j \textbf{s}_i - \textbf{s}_i^\ast U_j \textbf{s}_i \theta_i = 0$ (with real eigenvalues or matrices), which results in \autoref{eq:eig-orth-loss} after reaarangement. Since $G_j$ collects all the numerical uncertainties of the procedure at a given step it stands to reason that $\gamma_{ii} \approx \epsilon\left\lVert A \right\rVert$.
	
\end{proof}

Using \autoref{lemma:paige} we can see that those Ritz pairs who are closely approximating, meaning they have low error in $\beta_{ji} := \|A\textbf{y}_i - \theta_i \textbf{y}_i\|_2$, will correspond to a high $\left|\textbf{y}_i^{\ast}\textbf{q}_{j+1}\right|$. On the other side for those pairs which are far from converginfg this will be high. This means that $\textbf{q}_{k+1}$ only has high components in the well converged Ritz vectors' directions. This means that the subspace's orthogonality is lost in these directions for the next $q_{k+1}$, as $y_i = Q_k s_i \rightarrow y_i \in \mathrm{col}\left(Q_k\right)$.

\section{Eigenvalues of a tridiagonal matrix}
The ideas presented here are, as presumably the assignment is as well, losely based on the fundamental paper of \textcite{givens-eig} and later work of \textcite{sturm-ortega1960}.
\subsection{Characteristic polynomial}
Suppose that a tridiagonal symmetric (real) matrix $T$ is given:

$$T=\begin{pmatrix}
	\alpha_1&\beta_1&0&\cdots&0\\
	\beta_1&\alpha_2&\beta_2&&0\\
	\vdots&\ddots&\ddots&\ddots&\vdots\\
	0&&\beta_{j-2}&\alpha_{j-1}&\beta_{j-1}\\
	0&\cdots&0&\beta_{j-1}&\alpha_{j}\\
\end{pmatrix}$$

Let $T_j := T (1 : j, 1 : j)$ as before. Now let $p_j(x)=\det\left(x I - T_j\right)$ denote the characteristic polynomial of $T_j$ (with $p_0 = 1$). A reccurence relation can easily be established using the recursive nature of the determinant and the matrix. We can write the $j^{\mathrm{th}}$ characteristic polynomial as
\begin{equation}
	p_j(x)=\det\left(x I - T_j\right) = \left|\begin{NiceMatrix}
		x-\alpha_1&-\beta_1&0&\cdots&0\\
		-\beta_1&x-\alpha_2&-\beta_2&&0\\
		\vdots&\ddots&\ddots&\ddots&\vdots\\
		0&&-\beta_{j-2}&x-\alpha_{j-1}&-\beta_{j-1}\\
		0&\cdots&0&-\beta_{j-1}&x-\alpha_{j}
		\CodeAfter
		\begin{tikzpicture}[remember picture, overlay]
			%\draw[->] (1-1.-20) to[out=-25, in=30] (4-3.30);
			\draw[red, opacity=0.3, fill opacity=0.05, dashed,rounded corners, fill=red] ($(1-1)!1.3!(1-1.north west)$) rectangle ($(4-4)!1.3!(4-4.south east)$);
			\node[red, opacity=0.5] at ($(1-4)!0.5!(4-4)$) {$xI-T_{j-1}$};
			
			\draw[blue, opacity=0.3, fill opacity=0.05, dashed,rounded corners, fill=blue] (1-1.north west) rectangle (4-3.south east);
			\draw[blue, opacity=0.3, fill opacity=0.05, dashed,rounded corners, fill=blue] ($(1-5.north)!(4-5.south west)!(1-1.north)$) rectangle (4-5.south east);
			\node[blue, opacity=0.5] (Ujm) at ($(1-4)+(0,0.3)$) {$U_{j-1}$};
			\draw [<-, blue, opacity=0.2](Ujm.160) to[in=30, out=150] (1-3);
			\draw [<-, blue, opacity=0.2](Ujm.20) to[in=150, out=40] (1-5);
		\end{tikzpicture}
	\end{NiceMatrix}\right| = \left(x-\alpha_{j}\right) \underbrace{\det{\left(xI-T_{j-1}\right)}}_{p_{j-1}} - \left(-\beta_{j-1}\right) \det{\left(U_{j-1}\right)}
\end{equation}
where we just used the so called Laplace expansion on the matrix while looking at the two non-zero elements of the last row. Since the determinant of $U_{j-1}$ can also be simply written as
\begin{equation*}
	\det{\left(U_{j-1}\right)} = \left|\begin{NiceMatrix}
		x-\alpha_1&-\beta_1&0&\cdots&0\\
		-\beta_1&x-\alpha_2&-\beta_2&&0\\
		\vdots&\ddots&\ddots&\ddots&\vdots\\
		0&&-\beta_{j-3}&x-\alpha_{j-2}&0\\
		0&\cdots&0&-\beta_{j-2}&-\beta_{j-1}
		\CodeAfter
		\begin{tikzpicture}[remember picture, overlay]
			\draw[orange, opacity=0.3, fill opacity=0.05, dashed,rounded corners, fill=orange] ($(1-1)!1.3!(1-1.north west)$) rectangle ($(4-4)!1.3!(4-4.south east)$);
			\node[red, opacity=0.5] at ($(1-4)!0.5!(4-4)$) {$xI-T_{j-2}$};
		\end{tikzpicture}
	\end{NiceMatrix}\right| = -\beta_{j-1} \underbrace{\det\left(xI-T_{j-2}\right)}_{p_{j-2}}
\end{equation*}
by expanding using the last row. Therefore using the definition of the characteristic polynomial we have our recursion in the form of 
\begin{equation}\label{eq:char-rec}
	p_j(x) = (\alpha_j - x)p_{j-1} (x) - \beta_{j-1}^2
	p_{j-2}(x).
\end{equation}

Hence for any given value of $x$, the resulting sequence of characteristic polynomials can be calculated using \autoref{alg:char-poly-eval}, where to get $p_n$ a loop of scalar multiplication, thereby having a complexity of $\mathcal{O}(n)$.

\begin{algorithm2e}[ht]
	
	\SetKwInOut{Input}{input}
	\SetKwInOut{Output}{output}
	\SetKw{Init}{init}{}{}
	\SetAlgoLined
	\Input{Real symmetric tridiagonal matrix $T_j\in\mathbb{R}^{j\times j}$, given by diagonals $\mathbf{\alpha},\mathbf{\beta}$, $x$ real constant}
	\Output{$\left\{p_j\right\}_{j=0}^n$ vector of the values of the characteristic polynomials evaluated at $x$}
	\Init{Allocate the $\left\{p_j\right\}_{j=0}^n$ vector, set the trivial cases: $p_0=1$, $p_1=\alpha_1-x$}\\
	\For{$j=2,3,...,n$}{
		$p_j = (\alpha_j - x)p_{j-1} - \beta_{j-1}^2
		p_{j-2}$\\
	}
	\caption{Evaluation of the characteristic polynomials ($p_j$ for $j=1,2,\dots,k$)}\label{alg:char-poly-eval}
\end{algorithm2e}


\subsection{Finding the $k^{\mathrm{th}}$ eigenvalue}

Just by finding the zeros of the characteristic polynomial using the bisection method for example, by repeatedly evaluating it. This costs only $\mathcal{O}(n\log{n})$, disregarding some of the technical difficulties. With this algorithm however we would not be able to tell which eigenvalue we have converged to. 

For us to find a more useful algorithm we first have to remind ourselves of the Sturm sequence like property of the characteristic polynomials defined by
 \autoref{eq:char-rec}.
 \begin{theorem}[Sturm sequence property]
 	Suppose $T$ is such that $\{\beta_i\}_{i=1}^{n-1}$ contains no zeros. Then for any $j \in \{1,\ldots,n\}$, the eigenvalues of $T_j$ and $T_{j-1}$ interlace as follows:
 	$$\lambda_j(T_j) <\lambda_{j-1}(T_{j-1}) < \lambda_{j-1}(T_j) <\cdots< \lambda_2(T_j) <\lambda_1(T_{j-1}) < \lambda_1(T_j)$$.
 \end{theorem}
 
  \begin{figure}[h!]
 	\centering
 	\begin{tikzpicture}[line width=3pt]
 		% Define the list of lists
 		\def\listOfLists{{},{5},{3,6},{1.5,3.5,8},{1.3,2.5,4,8.5},{0.7,1.8,3,7,8.7},{0.3,1.4,2,4.5,8.4,9}}
 		
 		% Define the colors for alternating connections
 		\definecolor{color1}{RGB}{0, 0, 255}
 		\definecolor{color2}{RGB}{255, 0, 0}
 		\colorlet{linecolor}{color1}
 		% Define the vertical spacing between levels
 		\def\verticalSpacing{-0.65}
 		
 		% Initialize the y-coordinate
 		\pgfmathsetmacro{\ycoord}{0}
 		
 		% Loop through the list of lists
 		\newcounter{arraycard}
 		\foreach \level [count=\i] in \listOfLists {
 			% Get the length of the current \level
 			\setcounter{arraycard}{0}%
 			% Draw nodes for each element in the sublist
 			\foreach \x [count=\j] in \level {
 				\pgfmathparse{\ycoord + (\i - 1) * \verticalSpacing}
 				
 				\node[mark=square, draw, fill=black, inner sep=2pt] (node\i\j) at ($(\x, \pgfmathresult)$) {};
 			}
 			
 			% Connect nodes on the same level with alternating colors
 			
 			\foreach \x [count=\j] in \level {
 				\let\ycoord\pgfmathresult
 				\ifodd\j
 				\colorlet{linecolor}{color2}
 				\else
 				\colorlet{linecolor}{color1}
 				\fi
 				\ifnum\j>1
 				\draw[linecolor] (node\i\j) -- (node\i\the\numexpr\j-1\relax);
 				\fi
 				\stepcounter{arraycard}%
 			}
 			% Use \levelLength to access the length of the \level
 			%\node[above right] at (node\i\the\value{arraycard}) {\the\value{arraycard}\ elements};
 			\pgfmathparse{int(\i-1)}
 			\node[draw=none] (node\i0) at ($(0, \ycoord + \i* \verticalSpacing - 1 * \verticalSpacing)$) {};
 			\node (name\i) at ($(node\i0)+(-0.5,0)$) {$p_\pgfmathresult$};
 			\pgfmathparse{int(\the\value{arraycard}+1)}
 			\node (node\i\pgfmathresult) at ($(10, \ycoord + \i* \verticalSpacing - 1 * \verticalSpacing)$) {};
 			\draw[color2] (node\i1) --($(0, \ycoord + \i* \verticalSpacing - 1 * \verticalSpacing)$);
 			
 			\ifodd\the\value{arraycard}
 			\colorlet{linecolor}{color1}
 			\else
 			\colorlet{linecolor}{color2}
 			\fi
 			\pgfmathparse{int(\the\value{arraycard}+1)}
 			\draw[linecolor,->] (node\i\the\value{arraycard}) -- (node\i\pgfmathresult);
 		}
 	\end{tikzpicture}
 	
 	\caption{
 		Visualizing zeros of the different levels of characteristic polynomials and their interlaced zeros. The lines represent the real $x$ axis, indicating where the value of the polynomial is {\color{red}positive (red)} or {\color{blue}negative (blue)} or zero ($\blacksquare$)
 	}\label{fig:charpoly-zero}
 \end{figure}
 
 This means that we can construct a hierarchy of the zeros of these polynomials and use information about all of their signs to tell how many zeros fall below a given value. \autoref{fig:charpoly-zero} nicely illustrates this structure and leads us to a couple of observations. Let's denote the number of sign flips in the sequence of polynomilas (evaluated at a certain $x$) $p_n(x), p_{n-1}(x), \dots, p_1(x), p_0(x)$ by $s(x)$ . 
 \begin{enumerate}
 	\item At the negative side as $x \to -\infty$, we see that since the leading term of the $j^{\mathrm{th}}$ polynomial is $\left(-1\right)^j x_j$, therefore $p_j \to \infty$, and hence the sign will be positive for all polynomials at a sufficiently low $x$, which results in $s(x)\to0$. On the flip side, for sufficiently large $x$ however $s(x)$ as the signs will alternate due to the $\left(-1\right)^j$ term.
 	\item Coming from negative $x$ values, between two eigenvalues, i.e. zeros of $p_n$, the number sign flips cannot change. This is due to the propery of interlaced zeros: if $p_j(x)$ flips in sign, then $p_{j+1}$ must have had a zero before that which flipped it, i.e. it will have the same sign as $p_j(x)$.
 	\item This leads to the last conclusion: $p_n$ introduces sign flips (because there is no $p_{n+1}$) at each of it's zeros, which are the eigenvalues.
 \end{enumerate}
 
 Thereby we can conclude that $s(x)$ describes the number of eigenvalues of $T$, which are smaller than $x$. This leads to an algorithm based on the idea of the bisection method to find $\lambda_k = \inf_{x} \left\lbrace x | s(x)=k\right\rbrace$, which will correspond to the $k^{\mathrm{th}}$ eigenvalue of $T_j$.
 
 \begin{algorithm2e}[ht]
 	
 	\SetKwInOut{Input}{input}
 	\SetKwInOut{Output}{output}
 	\SetKw{Init}{init}{}{}
 	\SetAlgoLined
 	\Input{Real symmetric tridiagonal matrix $T_j\in\mathbb{R}^{j\times j}$, given by diagonals $\mathbf{\alpha},\mathbf{\beta}$, $k$ index ($k\leq j$)}
 	\Output{$\lambda_k$ eigenvalue of $T_j$}
 	\Init{$\beta_0 = \beta_{j} = 0$}\\
 	\tcc{Lower and upper bounds of the eigenvalues using the Gershgorin circle theorem}
 	$R_i = \left|\beta{i-1}\right|+\left|\beta{i}\right|$ \textbf{for} $i=1,2,\dots,j$\\
 	$a = \min_{1\leq i \leq j}{\left(\alpha_{i}-R_i\right)}$\\
 	$b = \max_{1\leq i \leq j}{\left(\alpha_{i}+R_i\right)}$\\
 	\tcc{Bisection method}
 	\While{$\frac{b-a}{b} > \kappa$}{
 		$c = \frac{a+b}{2}$\\
 		\tcc{Using \autoref{alg:char-poly-eval} to evaluate $s(c)$ in $\mathcal{O}(j)$}
 		\uIf{$s(c, \alpha, \beta) < k$}{$a=c$}
 		\Else{$b=c$}
 	}
 	$\lambda_k = \frac{a+b}{2}$
 	\caption{Finding the $k^{\mathrm{th}}$ eigenvalue of the tridiagonal matrix $T_j$ with $\kappa/2$ precision}\label{alg:eigfind-Tj}
 \end{algorithm2e}
 
Note that if a $\beta_l$ is zero, that will break up the matrix into two blocks. This leads to the natural conclusion that in case of zero betas we can just recursively apply the algorithm. In a slightly more sophisticated fashion we could generalize the algorithm \autoref{alg:eigfind-Tj} so that it searches the blocks separately until out of $b$ blocks you find the one the $k^{\mathrm{th}}$ eigenvalue of the whole matrix, i.e. the $b$ blocks collectively have $k-1$ less eigenvalues smaller.

\subsubsection*{Finding mutliple eigenvalues at the same time}

To be more optimal at recycling information, one could develop a recursive algorithm for calculating a set of desired eigenvalues, where each evaluation of $s(x)$ is used to its maximal potential. \autoref{alg:eigfind-Tj-recursive} descirbes such an algorithm. Here let us denote the set of whole numbers between $a$ and $b$ ($a<b$) with 
\begin{equation*}
	\Upsilon(a,b) = \left\lbrace a, a+1, a+2, \dots b-1, b\right\rbrace
\end{equation*}

\begin{algorithm2e}[ht]
	
	\SetKwInOut{Input}{input}
	\SetKwInOut{Output}{output}
	\SetKw{Init}{init}{}{}
	\SetAlgoLined
	\Input{Real symmetric tridiagonal matrix $T_j\in\mathbb{R}^{j\times j}$, given by diagonals $\mathbf{\alpha},\mathbf{\beta}$, $K$ set of integer indicies}
	\Output{$\left[\lambda_k\right]_{k \in K}$ eigenvalues of $T_j$, trying to preserve multiplicity}
	\Init{$\beta_0 = \beta_{j} = 0$}\\
	\tcc{Lower and upper bounds of the eigenvalues using the Gershgorin circle theorem}
	$R_i = \left|\beta{i-1}\right|+\left|\beta{i}\right|$ \textbf{for} $i=1,2,\dots,j$\\
	$a = \min_{1\leq i \leq j}{\left(\alpha_{i}-R_i\right)}$\\
	$b = \max_{1\leq i \leq j}{\left(\alpha_{i}+R_i\right)}$\\
	\Return{\texttt{SetBisect}$\left(K, a, b,\alpha, \beta\right)$}\\
	\tcc{Recursive bisection method}
	\SetKwFunction{FMain}{SetBisect}
	\SetKwProg{Fn}{Function}{:}{}
	\Fn{\FMain{$K, a, b, \alpha, \beta$}}{
		\uIf{$|K|=0$}{\Return{$[\quad]$\tcp{No eigenvalues in this interval to be found}}}
		\uElseIf{$\frac{b-a}{b} < 2\epsilon$}{
			\tcp{The eigenvalues could not be resolved any further}
			\Return{$\underbrace{\left[\frac{a+b}{2},\,\frac{a+b}{2},\,\dots,\frac{a+b}{2}\right]}_{\left|K\right| \,\mathrm{ elements}}$}
		}
		\uElseIf{$\frac{b-a}{b} > \kappa$ \textbf{or} $|K| > 1$}{
			$c = \frac{a+b}{2}$\\
			\tcc{Using \autoref{alg:char-poly-eval} to evaluate $s(c)$ in $\mathcal{O}(j)$. Values are cached in reality}
			\Return{\texttt{SetBisect}$\left(K \cap \Upsilon\left(s(a),s(c)\right),a,c, \alpha, \beta\right)\,$ $\cup\footnotemark\,$ \texttt{SetBisect}$\left(K \cap \Upsilon\left(s(c),s(b)\right),c, b, \alpha, \beta\right)$\tcp{Recursion}}
		}
		\Else{\Return{$\left[\frac{a+b}{2}\right]$}\tcp{Eigenvalue resolved with $\frac{\kappa}{2}$ precision in this interval}}
	}
	\textbf{End Function}
	\caption{Finding a set $K$ eigenvalues of the tridiagonal matrix $T_j$ with $\kappa/2$ precision}\label{alg:eigfind-Tj-recursive}
\end{algorithm2e}
\footnotetext{Interpret $\cup$ as concatenation for arrays}

\FloatBarrier
\newpage
\printbibliography
\end{document}